{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba4f071-9be2-4552-858d-f7c79b3f26da",
   "metadata": {},
   "source": [
    "# Predicting Titanic Survival Using Machine Learning\n",
    "**Author:** Justin Zhao\n",
    "\n",
    "```\n",
    "\"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\"\n",
    "```\n",
    "This is a Machine Learning challenge/competition from Kaggle: https://www.kaggle.com/competitions/titanic/overview\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates the end-to-end machine learning pipeline used to achieve a top 5% score on the Titanic Kaggle leaderboard.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Preprocessing:** Custom feature engineering (Title extraction, Family size binning) and robust imputation.\n",
    "2.  **Model Selection:** GridSearch tuning of three distinct classifiers:\n",
    "    * **XGBoost** (Gradient Boosting)\n",
    "    * **RandomForest** (Bagging)\n",
    "    * **Logistic Regression** (Linear baseline)\n",
    "3.  **Ensemble:** A `VotingClassifier` to combine predictions and reduce variance.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "> Given some information (name, age, gender, socio-economic class) of an individual aboard the RMS Titanic, can we predict whether or not they survived the shipwreck?\n",
    "\n",
    "## Data\n",
    "\n",
    "The data for this analysis was obtained from the Kaggle Titanic competition: \n",
    "\n",
    "https://www.kaggle.com/competitions/titanic/overview\n",
    "\n",
    "This dataset is widely believed to be a subset of the \"Titanic Passenger Survival Data\" originally compiled by Thomas Cason and hosted by the Department of Biostatistics at Vanderbilt University.\n",
    "\n",
    "It is important to note that while the data is based on the historical record from Encyclopedia Titanica, it is not an exhaustive list of everyone on board. Specifically, this dataset contains 1,309 records (split into train/test sets) and focuses exclusively on passengers, notably excluding the ~900 crew members who were also aboard the ship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e207236-a09c-4292-afc3-86cb077ee3ea",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "| Variable | Role | Type | Description | Units / Key | Missing Values |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **survival** | Target | Integer | Survival status | 0 = No, 1 = Yes | No |\n",
    "| **pclass** | Feature | Integer | Ticket class (Proxy for SES) | 1 = 1st, 2 = 2nd, 3 = 3rd | No |\n",
    "| **sex** | Feature | Object | Sex | male, female | No |\n",
    "| **Age** | Feature | Float | Age in years | Years | **Yes** |\n",
    "| **sibsp** | Feature | Integer | # of siblings / spouses aboard | Count | No |\n",
    "| **parch** | Feature | Integer | # of parents / children aboard | Count | No |\n",
    "| **ticket** | Feature | Object | Ticket number | Alphanumeric | No |\n",
    "| **fare** | Feature | Float | Passenger fare | Currency | No |\n",
    "| **cabin** | Feature | Object | Cabin number | Alphanumeric | **Yes** |\n",
    "| **embarked** | Feature | Object | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | **Yes** |\n",
    "\n",
    "* **survival**: This is the target variable that indicates whether the passenger survived (1) or died (0).\n",
    "* **pclass**: This represents the ticket class (1st, 2nd, or 3rd) and acts as a proxy for socio-economic status.\n",
    "* **sex**: This records the gender of the passenger.\n",
    "* **Age**: This is the passenger's age in years, which may be fractional for infants.\n",
    "* **sibsp**: This counts the total number of siblings and spouses traveling with the passenger.\n",
    "* **parch**: This counts the total number of parents and children traveling with the passenger.\n",
    "* **ticket**: This is the unique alphanumeric ticket number printed on the passenger's ticket.\n",
    "* **fare**: This records the monetary price the passenger paid for their journey.\n",
    "* **cabin**: This lists the specific cabin number assigned to the passenger, though many values are missing.\n",
    "* **embarked**: This indicates the port where the passenger boarded the ship (Cherbourg, Queenstown, or Southampton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247d5277-127f-41fb-92f8-fb95f919b44a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# regular EDA (exploratory data analysis) and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# data manipulation pipeline (imputing, encoding)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# models from Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# model evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce41f61e-0119-4246-bc4a-f598c43f27c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "df = pd.read_csv(\"data/titanic_train.csv\")\n",
    "df_test = pd.read_csv(\"data/titanic_test_no_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635205a-73e4-409b-9064-176b9354385f",
   "metadata": {},
   "source": [
    "### 1. Preprocessing & Feature Engineering\n",
    "\n",
    "Effective preprocessing was the primary driver of model performance in this project. Rather than throwing all features into the model at once, I adopted an iterative approach: establishing a baseline with simple imputation, and then sequentially adding engineered features to validate their impact on an **XGBoost** model.\n",
    "\n",
    "### 1.1 Feature Evolution & EDA Findings\n",
    "Below is a summary of the iterative improvements observed during the Exploratory Data Analysis phase.\n",
    "\n",
    "#### **Phase 1: Baseline**\n",
    "* **Strategy:** Basic categorical encoding of features `Sex`. Imputation of `Age` and `Fare` if necessary (not necessary for decision trees)\n",
    "* **Logic:** Establish a control score using only raw numerical data and basic categorical encoding.\n",
    "* **Cross-Validation Score:** `~82.3%`\n",
    "* **AUC:** `~0.84`\n",
    "\n",
    "#### **Phase 2: Title Extraction**\n",
    "* **Strategy:** Extracted prefixes from the `Name` column (e.g., Mr., Mrs., Master., Dr.). grouped rare titles (Don, Rev, Major) into a single `Rare` category. Clean up noisy/unecessary features such as `Ticket`.\n",
    "* **Logic:** Titles serve as a proxy for social status, gender, and age grouping simultaneously. 'Master' specifically helps identify male children who (unlike adult males) had high survival rates.\n",
    "* **Cross-Validation Score:** `~82.8%`\n",
    "* **AUC:** `~0.86`\n",
    "\n",
    "#### **Phase 3: Family Survival Rate**\n",
    "* **Strategy:** Calculated family sizes based on `Parch` and `Sibsp`; Calculated the survival rate of groups sharing the same ***Surname*** and `Ticket` price.\n",
    "* **Logic:** Survival was rarely independent; if one family member survived, others were significantly more likely to survive (and vice versa) due to rescue boats keeping groups together.\n",
    "* **Cross-Validation Score:** `~86.7%` *(+3.9% lift)*\n",
    "* **AUC:** `~0.92` *(+0.06 lift)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Final Preprocessing Pipeline\n",
    "Based on these findings, the final model incorporates all successful features. It uses a custom transformer pipeline to handle all data types and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa65a02-0b7c-4e8c-beaa-3a811f44e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import create_family_survival_feature\n",
    "X_train_full, X_test_full = create_family_survival_feature(df, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2eb3546-05db-4fe2-946f-c0721cd08f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "X = X_train_full.drop(\"Survived\", axis=1)\n",
    "y = X_train_full[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa6c1c1-8293-465f-ac19-565a2267fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import feat_eng_transformer\n",
    "\n",
    "feat_eng_functionTransformer = feat_eng_transformer()\n",
    "\n",
    "from src.preprocessing import title_transformer\n",
    "from src.preprocessing import sex_transformer\n",
    "from src.preprocessing import cabin_transformer\n",
    "\n",
    "clean_features = [\"Pclass\", \"Fare\", \"Age\", \"FamilySize\", \"Family_Survival\"]\n",
    "\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"sex\", sex_transformer(), [\"Sex\"]),\n",
    "        (\"cabin\", cabin_transformer(), [\"Cabin\"]),\n",
    "        (\"title\", title_transformer(), [\"Title\"]),\n",
    "        (\"clean_cols\", \"passthrough\", clean_features)\n",
    "    ], \n",
    "remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9777a2-558a-4785-95c2-532c0e433b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import age_binning_transformer\n",
    "from src.preprocessing import fare_bin_transformer\n",
    "from src.preprocessing import family_size_transformer\n",
    "from src.preprocessing import cat_transformer\n",
    "\n",
    "logreg_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"sex\", sex_transformer(), [\"Sex\"]),\n",
    "        (\"cabin\", cabin_transformer(), [\"Cabin\"]),\n",
    "        (\"title\", title_transformer(), [\"Title\"]),\n",
    "        (\"age\", age_binning_transformer(), [\"Age\"]),\n",
    "        (\"fams_size\", family_size_transformer(), [\"FamilySize\"]),\n",
    "        (\"fare\", fare_bin_transformer(), [\"Fare\"]),\n",
    "        (\"cat\", cat_transformer(), [\"Pclass\",\"Family_Survival\"])\n",
    "    ], \n",
    "remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e83fc5-f600-4b89-92b4-13dc1302e0e7",
   "metadata": {},
   "source": [
    "## 2. Model Training & Hyperparameter Tuning\n",
    "\n",
    "With the feature engineering finalized, we move to model selection and tuning. To ensure a robust final ensemble, I selected three distinct algorithms that learn from the data in fundamentally different ways: **Gradient Boosting (XGBoost)**, **Bagging (Random Forest)**, and a **Linear Baseline (Logistic Regression)**.\n",
    "\n",
    "### 2.1 Strategy: GridSearch & Cross-Validation\n",
    "I utilized `GridSearchCV` for hyperparameter tuning on each model. I trained and cross-validated each model to find the optimal balance between bias and variance.\n",
    "\n",
    "#### **A. Tree-Based Models (XGBoost & Random Forest)**\n",
    "* **Role:** These models are excellent at capturing non-linear relationships and interactions (e.g., *Age* vs *Class*) without explicit feature engineering.\n",
    "* **Preprocessing:** Tree models are generally robust to unscaled data and can handle raw feature distributions well.\n",
    "\n",
    "#### **B. The Linear Baseline (Logistic Regression)**\n",
    "* **Role:** Provides a probabilistic baseline. If a complex tree model can't beat this, the signal is likely linear.\n",
    "* **Pipeline Adaptation:** Unlike trees, Logistic Regression **required a distinct pipeline**.\n",
    "    * **Binning is Critical:** Features need to be either binned or scaled, features like *Age*, *Fare* need to be binned and imputed for LogReg. \n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Hyperparameter Tuning Implementation\n",
    "The code below defines the specific parameter grids for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e411898-a147-4bc5-a926-e5e71a17ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = [\n",
    "    {\n",
    "        'model': [XGBClassifier()],\n",
    "        'model__n_estimators': [800, 1000, 1200],\n",
    "        'model__learning_rate': [0.005, 0.007],\n",
    "        'model__max_depth': [3, 4],\n",
    "        'model__subsample': [0.5],\n",
    "        'model__colsample_bytree': [0.5],\n",
    "        'model__gamma': [0.1, 0.2],\n",
    "        'model__min_child_weight': [4, 5],\n",
    "        'model__reg_alpha' : [1],\n",
    "        'model__reg_lambda' : [2]\n",
    "    }\n",
    "]\n",
    "param_grid_rf = [\n",
    "    {\n",
    "        'model': [RandomForestClassifier()],\n",
    "        'model__n_estimators': [ 500, 800, 1000, 1200],\n",
    "        'model__max_depth': [2, 3,4],\n",
    "        'model__min_samples_split': [ 2,3,4, 5],\n",
    "        'model__min_samples_leaf': [3, 4, 6],\n",
    "        'model__bootstrap': [True]\n",
    "    }\n",
    "]\n",
    "param_grid_lr = [\n",
    "    {\n",
    "        'model': [LogisticRegression()],\n",
    "        'model__C': [ 0.002, 0.1, 1],\n",
    "        'model__solver': [\"lbfgs\", \"liblinear\"]\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
